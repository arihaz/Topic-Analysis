{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "import gensim, logging\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import multiprocessing\n",
    "from __future__ import division\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    result=text.replace('<br clear=\"none\">',' ').replace('</p>',' ').replace('<br>',' ').replace('<p>',' ').replace('\\\\',' ').replace('\\n',' ').replace('   ',' ').replace('  ',' ')\n",
    "    return re.sub(r'\\W+ ', '', result).upper()\n",
    "\n",
    "def report_to_wordlist(raw_review,remove_stopwords=True ):\n",
    "    #Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review, \"lxml\").get_text() \n",
    "    # Remove non-letters        \n",
    "    regex = re.compile(\"[',\\.!?;Â¿:()\\\"=-]\")\n",
    "    letters_only = re.sub(regex, \" \", review_text)\n",
    "    #Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split() \n",
    "    for i in range(len(words)):\n",
    "        words[i] = str(unicodedata.normalize('NFKD', words[i]).encode('ascii', 'ignore'))\n",
    "        words[i] = str(words[i])\n",
    "        #print(words[i])\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"french\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_soc= open('test_set/SOC','r').read().split('<<<<<<<<<<NEW>>>>>>>>>>')\n",
    "data_civ= open('test_set/CIV','r').read().split('<<<<<<<<<<NEW>>>>>>>>>>')\n",
    "data_com= open('test_set/COM','r').read().split('<<<<<<<<<<NEW>>>>>>>>>>')\n",
    "data_crim= open('test_set/CRIM','r').read().split('<<<<<<<<<<NEW>>>>>>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(processes=4)\n",
    "X= pool.map(report_to_wordlist,data_soc)\n",
    "print(0)\n",
    "X += pool.map(report_to_wordlist,data_civ)\n",
    "print(1)\n",
    "X += pool.map(report_to_wordlist,data_com)\n",
    "print(2)\n",
    "X += pool.map(report_to_wordlist,data_crim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.zeros(len(data_civ+data_com+data_crim+data_soc))\n",
    "Y[len(data_soc):len(data_soc)+len(data_com)]= np.ones(len(data_com))\n",
    "Y[len(data_soc)+len(data_com):len(data_soc)+len(data_com)+len(data_crim)]= 2*np.ones(len(data_crim))\n",
    "Y[len(data_soc)+len(data_com)+len(data_crim):len(data_soc)+len(data_com)+len(data_crim)+len(data_civ)]= 3*np.ones(len(data_civ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Spliting the set in training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec + Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain_Xtest= X_train+X_test\n",
    "all_data = [gensim.models.doc2vec.LabeledSentence(Xtrain_Xtest[i], tags=['SENT_%s' %i])for i in range(len(Xtrain_Xtest))]\n",
    "model_d2v = gensim.models.Doc2Vec(min_count=1, window=10, size=100, workers=7)\n",
    "model_d2v.build_vocab(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    print(epoch)\n",
    "    model_d2v.train(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = [model_d2v.docvecs['SENT_%s' % i] for i in range(len(X_train))]\n",
    "test = [model_d2v.docvecs['SENT_%s' % i] for i in range(len(X_train), len(X_train)+len(X_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"train.csv\", train, delimiter=\",\")\n",
    "np.savetxt(\"test.csv\", test, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 11.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'learning_rate': 0.5} with a score of 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import xgboost\n",
    "lr_range = [0.01,0.1,0.5,1]\n",
    "#estimator_range=range(80,150)\n",
    "\n",
    "#param_grid = dict(n_estimators = estimator_range, learning_rate=lr_range)\n",
    "param_grid = dict(learning_rate=lr_range)\n",
    "cv = StratifiedShuffleSplit(y_train, n_iter=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(xgboost.XGBClassifier(), param_grid=param_grid, cv=cv,verbose=1)\n",
    "grid.fit(np.array(train), y_train)\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.5, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xg = xgboost.XGBClassifier(n_estimators=100, learning_rate=0.5)\n",
    "model_xg.fit(np.array(train),y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred=model_xg.predict(np.array(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8722619047619048"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy=len([i for i in range(len(y_test)) if y_pred[i]==y_test[i]])/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify_document(doc):\n",
    "    w=report_to_wordlist(doc)\n",
    "    vec=model_d2v.infer_vector(w)\n",
    "    vec=np.array(vec).reshape((1,-1))\n",
    "    return model_xg.predict(np.array(vec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words= []\n",
    "for item in X:\n",
    "    all_words+=item\n",
    "all_words=list(set(all_words))\n",
    "dic_words={}\n",
    "for i in range(len(all_words)):\n",
    "    dic_words[all_words[i]]=i\n",
    "train_cnn=[]\n",
    "test_cnn=[]\n",
    "for item in X_train :\n",
    "    train_cnn.append([dic_words[w] for w in item])\n",
    "for item in X_test :\n",
    "    test_cnn.append([dic_words[w] for w in item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = len(all_words)\n",
    "maxlen = max([len(X[i]) for i in range(len(X))])\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "('x_train shape:', (19598, 16329))\n",
      "('x_test shape:', (8400, 16329))\n",
      "Build model...\n",
      "Train on 19598 samples, validate on 8400 samples\n",
      "Epoch 1/2\n",
      "19598/19598 [==============================] - 9524s - loss: 0.1697 - acc: 0.9366 - val_loss: 0.1344 - val_acc: 0.9511\n",
      "Epoch 2/2\n",
      "19598/19598 [==============================] - 9267s - loss: 0.1284 - acc: 0.9517 - val_loss: 0.1395 - val_acc: 0.9506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cb917e90>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(train_cnn, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(test_cnn, maxlen=maxlen)\n",
    "ytrain_cnn=to_categorical(y_train, nb_classes=4)\n",
    "ytest_cnn=to_categorical(y_test, nb_classes=4)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "#  embedding layer which maps the vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 border_mode='valid',\n",
    "                 activation='relu'))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a fully-cnnected hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We compute with softmax the probability of the four classes:\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, ytrain_cnn,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=epochs,\n",
    "          validation_data=(x_test, ytest_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.predict_classes(x_test[0].reshape((1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
